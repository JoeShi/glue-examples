{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudFront 日志 ETL\n",
    "这个试验将展示如果对 CloudFront 日志进行 ELT 操作，在开始前我们先从 [CloudFront Log Format](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat) 中了解日志的字段的含义。\n",
    "\n",
    "该 ELT 实验有两个目的：\n",
    "* 为每一条记录增加**国家**和**城市**信息。该任务可以通过查询IP数据库完成，此处我们使用 [IP2Location Lite](https://lite.ip2location.com/) 数据库。 \n",
    "* 为每一条记录增加**设备品牌**, **操作系统**, **操作系统版本**信息。该任务可以通过解析user-agent字段完成，此处我们使用第三方Python库[user_agents](https://github.com/selwin/python-user-agents)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 对源数据进行爬虫分析\n",
    "\n",
    "我们下载了一部分 CloudFront 日志，并且以 csv 文件的格式保存到了S3中。\n",
    "\n",
    "    s3://joeshi/customer/hanhui-dongyou/cloudfront/\n",
    "        \n",
    "爬虫会自动帮助我们分析数据结构，我们将结果保存到 Glue Date Catetegory 的数据库中，起名 `cloudfront_log`. 数据库中包含一张 `cloudfront` 的表.\n",
    "\n",
    "此处我们可以通过 **Athena** 预览数据.\n",
    "\n",
    "### 2. 准备实验\n",
    "\n",
    "我们需要在实验开始前加载如下内容：\n",
    "\n",
    "1. 导入 **user_agents**， **ip2location** 第三方 Python 库\n",
    "2. 导入 **pyspark** 中使用到的数据类型和 UDF \n",
    "3. 创建一个 **GlueContext**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1548337377007_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-24-192.us-west-2.compute.internal:20888/proxy/application_1548337377007_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-31-185.us-west-2.compute.internal:8042/node/containerlogs/container_1548337377007_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from ip2location import IP2Location\n",
    "from user_agents import parse\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField, Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 下载 IP2Location\n",
    "\n",
    "将 IP2Location Lite 数据库下载并加载到内存。 数据库的文件位置位于\n",
    "\n",
    "    s3://joeshi/PoC/Glue/artifacts/IP2LOCATION-LITE-DB5.IPV6.BIN\n",
    "    \n",
    "我们通过 AWS Python SDK 下载，并保存到 `/tmp` 目录下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('joeshi')\n",
    "localPath = '/tmp/DB.BIN'\n",
    "bucket.download_file('PoC/Glue/artifacts/IP2LOCATION-LITE-DB5.IPV6.BIN', localPath)\n",
    "database = IP2Location.IP2Location()\n",
    "database.open(localPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 创建解析user-agent字段的UDF\n",
    "\n",
    "在 CloudFront 日志中，`cs-user-agent` 字段表示 **user-agent**, 如下示例\n",
    "\n",
    "    Dalvik/2.1.0%2520(Linux;%2520U;%2520Android%25205.0;%2520Lenovo%2520K50-T5%2520Build/LRX21M)\n",
    "   \n",
    "在 CloudFront 日志中 `%2520` 表示空格，因此我们需要将其替换，然后作为`user_agents`库的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_ua(ua_string):\n",
    "    user_agent = parse(ua_string.replace(\"%2520\", \" \"))\n",
    "    print(str(user_agent))\n",
    "    return Row(\"ua_os_family\", \"ua_os_version\", \"ua_device_brand\")(user_agent.os.family, user_agent.os.version_string, user_agent.device.brand)\n",
    "\n",
    "\n",
    "ua_schema = StructType([\n",
    "    StructField(\"ua_os_family\", StringType(), False),\n",
    "    StructField(\"ua_os_version\", StringType(), False),\n",
    "    StructField(\"ua_device_brand\", StringType(), False)\n",
    "])\n",
    "\n",
    "chop_ua_udf = udf(chop_ua, ua_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 创建将 IP String 转化成 Numeric 类型的UDF\n",
    "\n",
    "使用 IP2Location Lite 数据库，根据 `c-ip` 字段查询该 IP 所在的国家、城市、经纬度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_c_ip(c_ip):\n",
    "    rec = database.get_all(c_ip)\n",
    "    return Row(\"country_short\",\n",
    "               \"country_long\",\n",
    "               \"city\",\n",
    "               \"latitude\",\n",
    "               \"longitude\"\n",
    "               )(rec.country_short,\n",
    "                 rec.country_long,\n",
    "                 rec.city,\n",
    "                 rec.latitude,\n",
    "                 rec.longitude)\n",
    "\n",
    "\n",
    "# 如果是某一个 Field 需要转化成多个 Column，使用 StructField来实现\n",
    "c_ip_schema = StructType([\n",
    "    StructField(\"country_short\", StringType(), False),\n",
    "    StructField(\"country_long\", StringType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "chop_c_ip_udf = udf(chop_c_ip, c_ip_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 检查 Glue Crawler 爬虫分析的表结构\n",
    "\n",
    "将 Glue 中的 `cloudfront` 表的表结果打印出来，并检查结构是否正确。\n",
    "\n",
    "创建一个Glue **DynamicFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:  171866\n",
      "root\n",
      "|-- date: string\n",
      "|-- time: string\n",
      "|-- x_edge_location: string\n",
      "|-- sc_bytes: long\n",
      "|-- c_ip: string\n",
      "|-- cs_method: string\n",
      "|-- cs_host: string\n",
      "|-- cs_uri_stem: string\n",
      "|-- sc_status: long\n",
      "|-- cs_referer: string\n",
      "|-- cs_user_agent: string\n",
      "|-- cs_uri_query: string\n",
      "|-- cs_cookie: string\n",
      "|-- x_edge_result_type: string\n",
      "|-- x_edge_request_id: string\n",
      "|-- x_host_header: string\n",
      "|-- cs_protocol: string\n",
      "|-- cs_bytes: long\n",
      "|-- time_taken: double\n",
      "|-- x_forwarded_for: string\n",
      "|-- ssl_protocol: string\n",
      "|-- ssl_cipher: string\n",
      "|-- x_edge_response_result_type: string\n",
      "|-- cs_protocol_version: string\n",
      "|-- mbps: string"
     ]
    }
   ],
   "source": [
    "cf_logs = glueContext.create_dynamic_frame.from_catalog(database=\"cloudfront_log\", table_name=\"cloudfront\")\n",
    "print \"Counts: \", cf_logs.count()\n",
    "cf_logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 为记录增加 user agent 和地址位置信息\n",
    "\n",
    "将 DynamicFrame 转化成 pyspark 中的 DataFrame, 并且通过 **withColumn** 和 **udf** 增加列。\n",
    "\n",
    "打印检查新生成的 schema。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x_edge_location: string (nullable = true)\n",
      " |-- sc_bytes: long (nullable = true)\n",
      " |-- c_ip: string (nullable = true)\n",
      " |-- cs_uri_stem: string (nullable = true)\n",
      " |-- cs_user_agent: string (nullable = true)\n",
      " |-- x_edge_result_type: string (nullable = true)\n",
      " |-- time_taken: double (nullable = true)\n",
      " |-- x_edge_response_result_type: string (nullable = true)\n",
      " |-- ua_os_family: string (nullable = true)\n",
      " |-- ua_os_version: string (nullable = true)\n",
      " |-- ua_device_brand: string (nullable = true)\n",
      " |-- country_short: string (nullable = true)\n",
      " |-- country_long: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "cf_logs_df = cf_logs.toDF()\n",
    "cf_logs_df = cf_logs_df.withColumn(\"ua\", chop_ua_udf(cf_logs_df.cs_user_agent))\\\n",
    "    .withColumn(\"c_ip_rec\", chop_c_ip_udf(cf_logs_df.c_ip))\n",
    "cf_logs_df = cf_logs_df.select(\"x_edge_location\", \"sc_bytes\", \"c_ip\", \"cs_uri_stem\", \"cs_user_agent\", \"x_edge_result_type\",\n",
    "                       \"time_taken\", \"x_edge_response_result_type\", \"ua.*\", \"c_ip_rec.*\")\n",
    "cf_logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. (可选)预览结果\n",
    "\n",
    "显示前10条结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_logs_df.show(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 将清洗完的结果输出\n",
    "\n",
    "将清洗完的结果输出到 `s3://joeshi/PoC/Glue/job/cloudfront_etl_test/`;\n",
    "\n",
    "建议使用 **Parquet** 或者 **ORC** 作为输出格式，这种列式存储的文件更适合大数据的查询."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<awsglue.dynamicframe.DynamicFrame object at 0x7facc0b00650>"
     ]
    }
   ],
   "source": [
    "cf_logs = DynamicFrame.fromDF(cf_logs_df, glueContext, \"cloudfront_parquet\")\n",
    "glueContext.write_dynamic_frame.from_options(frame = cf_logs,\n",
    "                                             connection_type = \"s3\",\n",
    "                                             connection_options = {\"path\": \"s3://joeshi/PoC/Glue/job/cloudfront_etl_test/\"},\n",
    "                                             format = \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验结束"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
